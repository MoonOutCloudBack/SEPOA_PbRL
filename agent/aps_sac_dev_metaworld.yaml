# @package agent
_target_: agent.aps_sac_dev.APSSACAgent
name: aps_sac_metaworld
reward_free: ${reward_free}
obs_type: ??? # to be specified later
obs_shape: ??? # to be specified later
action_shape: ??? # to be specified later
device: ${device}
lr: 1e-4
critic_target_tau: 0.01
update_every_steps: 2
use_tb: ${use_tb}
use_wandb: ${use_wandb}
num_expl_steps: ??? # to be specified later
hidden_dim: 1024
feature_dim: 50
# stddev_schedule: 0.2
# stddev_clip: 0.3
sf_dim: 10
update_task_every_step: 5
# nstep: 3
# batch_size: 1024
# init_critic: true
init_all: false
knn_rms: true
knn_k: 12
knn_avg: true
knn_clip: 0.0001
num_init_steps: 4096 # set to ${num_train_frames} to disable finetune policy parameters
lstsq_batch_size: 4096
update_encoder: ${update_encoder}

# update meta, by sample a Gaussian distribution with mean = _regress_meta()
meta_variance: 0.3  

# large state entropy intrinsic reward
ent_reward_ratio: 5

# for sac
discount: 0.99
init_temperature: 0.1
alpha_lr: 1e-4
alpha_betas: [0.9, 0.999]
actor_lr: 1e-4
actor_betas: [0.9, 0.999]
actor_update_frequency: 1
critic_lr: 1e-4
critic_betas: [0.9, 0.999]
critic_tau: 0.005
critic_target_update_frequency: 2
batch_size: 512 # 1024 for Walker, 512 for Meta-world
learnable_temperature: true

lr_aps: 1e-4

critic_cfg:
  class: agent.utils.critic.DoubleQCritic
  # obs_dim: ??? # to be specified later
  # action_dim: ??? # to be specified later
  hidden_dim: 256
  hidden_depth: 3
    
actor_cfg:
  class: agent.utils.actor.DiagGaussianActor
  # obs_dim: ??? # to be specified later
  # action_dim: ??? # to be specified later
  hidden_depth: 3
  hidden_dim: 256
  log_std_bounds: [-5, 2]


